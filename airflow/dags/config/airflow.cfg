[core]
# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository
dags_folder = /opt/airflow/dags

plugins_folder = /opt/airflow/plugins

# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Whether to load the examples that ship with Airflow. It's good to
# get started, but you probably want to set this to False in a production
# environment
load_examples = False

# Logging level
logging_level = INFO

# Logging format
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s

# Whether to expose the webserver UI via Flask development server or a production-ready
# WSGI server
expose_config = False

# The executor class that airflow should use
executor = LocalExecutor

# The amount of parallelism as a setting to the executor
parallelism = 4

# The number of task instances allowed to run concurrently by the scheduler
max_active_tasks_per_dag = 12

# Whether to load the DAG examples
load_examples = False

# The DAGs folder is scanned for new files, this setting determines how often
dag_dir_list_interval = 30

[webserver]
# The base url of your website as airflow cannot guess what domain or
# cname you are using. This is used in emails that
# notify users of success/failure of jobs
base_url = http://localhost:8080

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Paths to the SSL certificate and key for the web server. When both are
# provided SSL will be enabled. This does not change the web server port.
web_server_ssl_cert =
web_server_ssl_key =

# Number of seconds the webserver waits before timing out.
web_server_worker_timeout = 120

# Number of workers to refresh the webserver
workers = 4

# Secret key used to run your flask app
secret_key = temporary_key

# Number of workers to run the Gunicorn web server
workers = 4

# Access log format
access_logformat = %%(h)s %%(l)s %%(u)s %%(t)s "%%(r)s" %%(s)s %%(b)s %%(L)s %%(f)s

# Expose the configuration file in the web server
expose_config = False

# Default user role
default_ui_timezone = UTC

# Whether to enable authentication or not
authenticate = True

# Filter the list of dags by owner name
filter_by_owner = False

[scheduler]
# The scheduler class that airflow should use
job_heartbeat_sec = 5

# The Scheduler will run a periodic job that will try to clean up the dag_runs
dagbag_import_timeout = 30

# How many threads to run
max_threads = 2

[admin]
# UI to hide sensitive variables
hide_sensitive_variable_fields = True

[smtp]
# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_port = 25
smtp_mail_from = airflow@example.com

[celery]
# WARNING: This section is only used if you install the celery packages and enable the
# CeleryExecutor in airflow.cfg.

[celery_broker_transport_options]
# WARNING: This section is only used if you install the celery packages and enable the
# CeleryExecutor in airflow.cfg.

[dask]
# WARNING: This section is only used if you install the dask packages and enable the
# DaskExecutor in airflow.cfg.

[kubernetes]
# WARNING: This section is only used if you enable the KubernetesExecutor in airflow.cfg.

[elasticsearch]
# WARNING: This section is only used if you enable elastic search logging handler.

[kubernetes_executor]
# WARNING: This section is only used if you enable the KubernetesExecutor in airflow.cfg.

[elasticsearch_configs]
# WARNING: This section is only used if you enable elastic search logging handler.